{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Script Description\n",
    "\n",
    "This Python script performs data processing and visualization using PySpark and Matplotlib. Below is a step-by-step breakdown of the script's functionality:\n",
    "\n",
    "1. **Initialize a Spark Session**:\n",
    "    - The script begins by initializing a Spark session using the `SparkSession.builder`. It configures various Spark settings:\n",
    "      - `spark.driver.host` is set to `localhost` to bind the driver to the local machine.\n",
    "      - `spark.executor.extraJavaOptions` is configured to optimize garbage collection with `G1GC` and to adjust the heap occupancy threshold.\n",
    "      - Memory allocations for the Spark driver and executors are set to 16GB and 8GB, respectively.\n",
    "      - `spark.driver.maxResultSize` is set to 4GB to limit the size of data collected to the driver.\n",
    "      - `spark.network.timeout` is extended to 1200 seconds to prevent network timeout errors during long-running tasks.\n",
    "      - The number of shuffle partitions is set to 200, which helps in distributing large datasets across multiple partitions.\n",
    "\n",
    "2. **Read a Parquet File**:\n",
    "    - The script reads data from a Parquet file located at `datasets\\yellow_tripdata\\yellow_tripdata_2023-06.parquet`.\n",
    "    - The DataFrame is repartitioned into 10 partitions and persisted in memory to optimize further operations.\n",
    "\n",
    "3. **Display the Schema**:\n",
    "    - The schema of the loaded DataFrame is printed to the console to provide an overview of the data structure, including column names and data types.\n",
    "\n",
    "4. **Count the Total Number of Records**:\n",
    "    - The script counts the total number of records (rows) in the DataFrame and prints this value to the console.\n",
    "\n",
    "5. **Convert to Pandas DataFrame**:\n",
    "    - The Spark DataFrame is converted to a Pandas DataFrame to facilitate plotting with Matplotlib. This step is necessary because Matplotlib operates on Pandas DataFrames.\n",
    "\n",
    "6. **Plot 1: Trip Distance vs. Total Amount (All Data)**:\n",
    "    - A scatter plot is created to visualize the relationship between `trip_distance` and `total_amount` for all the data points.\n",
    "    - The plot is saved as `trip_distance_vs_total_amount_all_data.png` to the local file system.\n",
    "\n",
    "7. **Calculate Mean and Standard Deviation**:\n",
    "    - The script calculates the mean and standard deviation for the `trip_distance` and `total_amount` columns.\n",
    "\n",
    "8. **Filter Data Based on Standard Deviations**:\n",
    "    - A filtered Pandas DataFrame, `normalized_pdf`, is created by including only the data points where both `trip_distance` and `total_amount` are within 2 standard deviations of their respective means.\n",
    "\n",
    "9. **Plot 2: Trip Distance vs. Total Amount (Normalized Data)**:\n",
    "    - Another scatter plot is generated to visualize the relationship between `trip_distance` and `total_amount`, but this time only for the normalized data.\n",
    "    - This plot is saved as `trip_distance_vs_total_amount_normalized_data.png`.\n",
    "\n",
    "10. **Stop the Spark Session**:\n",
    "    - Finally, the script stops the Spark session to release resources.\n",
    "\n",
    "This script is useful for analyzing and visualizing large datasets by leveraging the distributed computing capabilities of PySpark and the flexible plotting functions of Matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "Total number of records: 3307234\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize a Spark session\n",
    "# local[*] = run local with as many threads as cores available\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .appName(\"Read Parquet File\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the Parquet file\n",
    "# parquet_file = \"./datasets/combined_datasets/yellow_tripdata_2023.parquet\" # too large apparently\n",
    "parquet_file = \"./datasets/filtered_fields_datasets/yellow_tripdata_2023_06_distance_amount.parquet\"\n",
    "df = spark.read.parquet(parquet_file).repartition(10).persist()\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Get the total number of records\n",
    "total_records = df.count()\n",
    "\n",
    "print(f\"Total number of records: {total_records}\")\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Plot 1: All data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pdf['trip_distance'], pdf['total_amount'], alpha=0.5, color='blue')\n",
    "plt.title('Trip Distance vs Total Amount (All Data)')\n",
    "plt.xlabel('Trip Distance (miles)')\n",
    "plt.ylabel('Total Amount ($)')\n",
    "plt.grid(True)\n",
    "plt.savefig('trip_distance_vs_total_amount_all_data.png')  # Save the plot as a PNG file\n",
    "plt.close()  # Close the plot to free memory\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_trip_distance = pdf['trip_distance'].mean()\n",
    "std_trip_distance = pdf['trip_distance'].std()\n",
    "\n",
    "mean_total_amount = pdf['total_amount'].mean()\n",
    "std_total_amount = pdf['total_amount'].std()\n",
    "\n",
    "# Filter data to include only values within 2 standard deviations from the mean\n",
    "normalized_pdf = pdf[\n",
    "    (pdf['trip_distance'] > mean_trip_distance - 2 * std_trip_distance) & \n",
    "    (pdf['trip_distance'] < mean_trip_distance + 2 * std_trip_distance) &\n",
    "    (pdf['total_amount'] > mean_total_amount - 2 * std_total_amount) &\n",
    "    (pdf['total_amount'] < mean_total_amount + 2 * std_total_amount)\n",
    "]\n",
    "\n",
    "# Plot 2: Normalized data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(normalized_pdf['trip_distance'], normalized_pdf['total_amount'], alpha=0.5, color='green')\n",
    "plt.title('Trip Distance vs Total Amount (Normalized Data)')\n",
    "plt.xlabel('Trip Distance (miles)')\n",
    "plt.ylabel('Total Amount ($)')\n",
    "plt.grid(True)\n",
    "plt.savefig('trip_distance_vs_total_amount_normalized_data.png')  # Save the plot as a PNG file\n",
    "plt.close()  # Close the plot to free memory\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
